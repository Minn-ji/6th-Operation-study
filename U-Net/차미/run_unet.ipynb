{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{},"colab_type":"code","id":"Y8ter2UpdnFC"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{},"colab_type":"code","id":"qkaSfws4RIHf"},"outputs":[{"data":{"application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["tensorboard --logdir='/home/work/deep/xai/겨울방학스터디/UNet/log'"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{},"colab_type":"code","id":"4ZtjGhZhdmBE"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-24 11:18:45.050963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-24 11:18:45.051064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-24 11:18:45.052459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-24 11:18:45.059588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-24 11:18:46.033783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-03\n","batch size: 4\n","number of epoch: 100\n","data dir: ./datasets\n","ckpt dir: ./checkpoint\n","log dir: ./log\n","result dir: ./result\n","mode: train\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0006 | LOSS 0.8007\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0006 | LOSS 0.7329\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0006 | LOSS 0.6928\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0006 | LOSS 0.6650\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0006 | LOSS 0.6436\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0006 | LOSS 0.6272\n","VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0001 | LOSS 0.6646\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0006 | LOSS 0.5212\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0006 | LOSS 0.5029\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0006 | LOSS 0.4916\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0006 | LOSS 0.4779\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0006 | LOSS 0.4686\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0006 | LOSS 0.4634\n","VALID: EPOCH 0002 / 0100 | BATCH 0001 / 0001 | LOSS 0.5038\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0001 / 0006 | LOSS 0.4263\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0002 / 0006 | LOSS 0.4132\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0003 / 0006 | LOSS 0.4086\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0004 / 0006 | LOSS 0.4026\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0005 / 0006 | LOSS 0.3998\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0006 / 0006 | LOSS 0.3965\n","VALID: EPOCH 0003 / 0100 | BATCH 0001 / 0001 | LOSS 0.4269\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0001 / 0006 | LOSS 0.3634\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0002 / 0006 | LOSS 0.3644\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0003 / 0006 | LOSS 0.3661\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0004 / 0006 | LOSS 0.3642\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0005 / 0006 | LOSS 0.3643\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0006 / 0006 | LOSS 0.3633\n","VALID: EPOCH 0004 / 0100 | BATCH 0001 / 0001 | LOSS 0.3683\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0001 / 0006 | LOSS 0.3396\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0002 / 0006 | LOSS 0.3415\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0003 / 0006 | LOSS 0.3415\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0004 / 0006 | LOSS 0.3405\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0005 / 0006 | LOSS 0.3434\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0006 / 0006 | LOSS 0.3427\n","VALID: EPOCH 0005 / 0100 | BATCH 0001 / 0001 | LOSS 0.3520\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0001 / 0006 | LOSS 0.3333\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0002 / 0006 | LOSS 0.3275\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0003 / 0006 | LOSS 0.3298\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0004 / 0006 | LOSS 0.3264\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0005 / 0006 | LOSS 0.3240\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0006 / 0006 | LOSS 0.3213\n","VALID: EPOCH 0006 / 0100 | BATCH 0001 / 0001 | LOSS 0.3214\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0001 / 0006 | LOSS 0.3026\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0002 / 0006 | LOSS 0.3054\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0003 / 0006 | LOSS 0.3035\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0004 / 0006 | LOSS 0.3029\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0005 / 0006 | LOSS 0.3034\n","TRAIN: EPOCH 0007 / 0100 | BATCH 0006 / 0006 | LOSS 0.3047\n","VALID: EPOCH 0007 / 0100 | BATCH 0001 / 0001 | LOSS 0.3097\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0001 / 0006 | LOSS 0.3013\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0002 / 0006 | LOSS 0.2998\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0003 / 0006 | LOSS 0.2950\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0004 / 0006 | LOSS 0.2946\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0005 / 0006 | LOSS 0.2923\n","TRAIN: EPOCH 0008 / 0100 | BATCH 0006 / 0006 | LOSS 0.2936\n","VALID: EPOCH 0008 / 0100 | BATCH 0001 / 0001 | LOSS 0.2794\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0001 / 0006 | LOSS 0.2817\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0002 / 0006 | LOSS 0.2849\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0003 / 0006 | LOSS 0.2813\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0004 / 0006 | LOSS 0.2823\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0005 / 0006 | LOSS 0.2823\n","TRAIN: EPOCH 0009 / 0100 | BATCH 0006 / 0006 | LOSS 0.2802\n","VALID: EPOCH 0009 / 0100 | BATCH 0001 / 0001 | LOSS 0.2891\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0001 / 0006 | LOSS 0.2748\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0002 / 0006 | LOSS 0.2685\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0003 / 0006 | LOSS 0.2641\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0004 / 0006 | LOSS 0.2673\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0005 / 0006 | LOSS 0.2677\n","TRAIN: EPOCH 0010 / 0100 | BATCH 0006 / 0006 | LOSS 0.2683\n","VALID: EPOCH 0010 / 0100 | BATCH 0001 / 0001 | LOSS 0.2697\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0001 / 0006 | LOSS 0.2653\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0002 / 0006 | LOSS 0.2644\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0003 / 0006 | LOSS 0.2637\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0004 / 0006 | LOSS 0.2614\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0005 / 0006 | LOSS 0.2602\n","TRAIN: EPOCH 0011 / 0100 | BATCH 0006 / 0006 | LOSS 0.2585\n","VALID: EPOCH 0011 / 0100 | BATCH 0001 / 0001 | LOSS 0.2918\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0001 / 0006 | LOSS 0.2462\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0002 / 0006 | LOSS 0.2613\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0003 / 0006 | LOSS 0.2530\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0004 / 0006 | LOSS 0.2521\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0005 / 0006 | LOSS 0.2529\n","TRAIN: EPOCH 0012 / 0100 | BATCH 0006 / 0006 | LOSS 0.2519\n","VALID: EPOCH 0012 / 0100 | BATCH 0001 / 0001 | LOSS 0.2548\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0001 / 0006 | LOSS 0.2359\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0002 / 0006 | LOSS 0.2398\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0003 / 0006 | LOSS 0.2434\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0004 / 0006 | LOSS 0.2454\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0005 / 0006 | LOSS 0.2453\n","TRAIN: EPOCH 0013 / 0100 | BATCH 0006 / 0006 | LOSS 0.2445\n","VALID: EPOCH 0013 / 0100 | BATCH 0001 / 0001 | LOSS 0.2414\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0001 / 0006 | LOSS 0.2414\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0002 / 0006 | LOSS 0.2363\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0003 / 0006 | LOSS 0.2376\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0004 / 0006 | LOSS 0.2423\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0005 / 0006 | LOSS 0.2419\n","TRAIN: EPOCH 0014 / 0100 | BATCH 0006 / 0006 | LOSS 0.2399\n","VALID: EPOCH 0014 / 0100 | BATCH 0001 / 0001 | LOSS 0.2181\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0001 / 0006 | LOSS 0.2284\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0002 / 0006 | LOSS 0.2276\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0003 / 0006 | LOSS 0.2286\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0004 / 0006 | LOSS 0.2315\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0005 / 0006 | LOSS 0.2321\n","TRAIN: EPOCH 0015 / 0100 | BATCH 0006 / 0006 | LOSS 0.2318\n","VALID: EPOCH 0015 / 0100 | BATCH 0001 / 0001 | LOSS 0.2373\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0001 / 0006 | LOSS 0.2199\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0002 / 0006 | LOSS 0.2157\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0003 / 0006 | LOSS 0.2191\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0004 / 0006 | LOSS 0.2240\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0005 / 0006 | LOSS 0.2235\n","TRAIN: EPOCH 0016 / 0100 | BATCH 0006 / 0006 | LOSS 0.2236\n","VALID: EPOCH 0016 / 0100 | BATCH 0001 / 0001 | LOSS 0.2158\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0001 / 0006 | LOSS 0.2298\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0002 / 0006 | LOSS 0.2213\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0003 / 0006 | LOSS 0.2203\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0004 / 0006 | LOSS 0.2185\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0005 / 0006 | LOSS 0.2206\n","TRAIN: EPOCH 0017 / 0100 | BATCH 0006 / 0006 | LOSS 0.2212\n","VALID: EPOCH 0017 / 0100 | BATCH 0001 / 0001 | LOSS 0.2130\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0001 / 0006 | LOSS 0.2233\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0002 / 0006 | LOSS 0.2166\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0003 / 0006 | LOSS 0.2245\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0004 / 0006 | LOSS 0.2208\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0005 / 0006 | LOSS 0.2196\n","TRAIN: EPOCH 0018 / 0100 | BATCH 0006 / 0006 | LOSS 0.2202\n","VALID: EPOCH 0018 / 0100 | BATCH 0001 / 0001 | LOSS 0.2087\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0001 / 0006 | LOSS 0.2243\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0002 / 0006 | LOSS 0.2198\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0003 / 0006 | LOSS 0.2160\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0004 / 0006 | LOSS 0.2205\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0005 / 0006 | LOSS 0.2184\n","TRAIN: EPOCH 0019 / 0100 | BATCH 0006 / 0006 | LOSS 0.2165\n","VALID: EPOCH 0019 / 0100 | BATCH 0001 / 0001 | LOSS 0.2007\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0001 / 0006 | LOSS 0.2163\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0002 / 0006 | LOSS 0.2149\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0003 / 0006 | LOSS 0.2141\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0004 / 0006 | LOSS 0.2164\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0005 / 0006 | LOSS 0.2161\n","TRAIN: EPOCH 0020 / 0100 | BATCH 0006 / 0006 | LOSS 0.2144\n","VALID: EPOCH 0020 / 0100 | BATCH 0001 / 0001 | LOSS 0.2083\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0001 / 0006 | LOSS 0.1984\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0002 / 0006 | LOSS 0.2052\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0003 / 0006 | LOSS 0.2047\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0004 / 0006 | LOSS 0.2121\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0005 / 0006 | LOSS 0.2109\n","TRAIN: EPOCH 0021 / 0100 | BATCH 0006 / 0006 | LOSS 0.2142\n","VALID: EPOCH 0021 / 0100 | BATCH 0001 / 0001 | LOSS 0.2137\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0001 / 0006 | LOSS 0.2186\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0002 / 0006 | LOSS 0.2141\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0003 / 0006 | LOSS 0.2176\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0004 / 0006 | LOSS 0.2167\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0005 / 0006 | LOSS 0.2150\n","TRAIN: EPOCH 0022 / 0100 | BATCH 0006 / 0006 | LOSS 0.2154\n","VALID: EPOCH 0022 / 0100 | BATCH 0001 / 0001 | LOSS 0.2029\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0001 / 0006 | LOSS 0.1985\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0002 / 0006 | LOSS 0.2036\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0003 / 0006 | LOSS 0.2029\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0004 / 0006 | LOSS 0.2050\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0005 / 0006 | LOSS 0.2072\n","TRAIN: EPOCH 0023 / 0100 | BATCH 0006 / 0006 | LOSS 0.2057\n","VALID: EPOCH 0023 / 0100 | BATCH 0001 / 0001 | LOSS 0.2053\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0001 / 0006 | LOSS 0.2039\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0002 / 0006 | LOSS 0.2035\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0003 / 0006 | LOSS 0.2123\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0004 / 0006 | LOSS 0.2090\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0005 / 0006 | LOSS 0.2061\n","TRAIN: EPOCH 0024 / 0100 | BATCH 0006 / 0006 | LOSS 0.2077\n","VALID: EPOCH 0024 / 0100 | BATCH 0001 / 0001 | LOSS 0.1988\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0001 / 0006 | LOSS 0.1985\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0002 / 0006 | LOSS 0.2008\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0003 / 0006 | LOSS 0.1998\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0004 / 0006 | LOSS 0.1988\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0005 / 0006 | LOSS 0.1991\n","TRAIN: EPOCH 0025 / 0100 | BATCH 0006 / 0006 | LOSS 0.2028\n","VALID: EPOCH 0025 / 0100 | BATCH 0001 / 0001 | LOSS 0.1896\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0001 / 0006 | LOSS 0.1978\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0002 / 0006 | LOSS 0.2003\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0003 / 0006 | LOSS 0.2005\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0004 / 0006 | LOSS 0.1997\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0005 / 0006 | LOSS 0.2022\n","TRAIN: EPOCH 0026 / 0100 | BATCH 0006 / 0006 | LOSS 0.1997\n","VALID: EPOCH 0026 / 0100 | BATCH 0001 / 0001 | LOSS 0.1895\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0001 / 0006 | LOSS 0.2097\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0002 / 0006 | LOSS 0.2044\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0003 / 0006 | LOSS 0.2029\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0004 / 0006 | LOSS 0.1999\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0005 / 0006 | LOSS 0.2015\n","TRAIN: EPOCH 0027 / 0100 | BATCH 0006 / 0006 | LOSS 0.2003\n","VALID: EPOCH 0027 / 0100 | BATCH 0001 / 0001 | LOSS 0.2073\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0001 / 0006 | LOSS 0.2050\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0002 / 0006 | LOSS 0.1963\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0003 / 0006 | LOSS 0.1932\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0004 / 0006 | LOSS 0.1947\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0005 / 0006 | LOSS 0.1951\n","TRAIN: EPOCH 0028 / 0100 | BATCH 0006 / 0006 | LOSS 0.1952\n","VALID: EPOCH 0028 / 0100 | BATCH 0001 / 0001 | LOSS 0.1837\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0001 / 0006 | LOSS 0.2093\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0002 / 0006 | LOSS 0.2031\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0003 / 0006 | LOSS 0.2007\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0004 / 0006 | LOSS 0.1955\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0005 / 0006 | LOSS 0.1955\n","TRAIN: EPOCH 0029 / 0100 | BATCH 0006 / 0006 | LOSS 0.1936\n","VALID: EPOCH 0029 / 0100 | BATCH 0001 / 0001 | LOSS 0.1818\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0001 / 0006 | LOSS 0.1974\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0002 / 0006 | LOSS 0.1926\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0003 / 0006 | LOSS 0.1928\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0004 / 0006 | LOSS 0.1927\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0005 / 0006 | LOSS 0.1976\n","TRAIN: EPOCH 0030 / 0100 | BATCH 0006 / 0006 | LOSS 0.1967\n","VALID: EPOCH 0030 / 0100 | BATCH 0001 / 0001 | LOSS 0.1967\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0001 / 0006 | LOSS 0.2127\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0002 / 0006 | LOSS 0.2011\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0003 / 0006 | LOSS 0.1974\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0004 / 0006 | LOSS 0.1954\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0005 / 0006 | LOSS 0.1960\n","TRAIN: EPOCH 0031 / 0100 | BATCH 0006 / 0006 | LOSS 0.1975\n","VALID: EPOCH 0031 / 0100 | BATCH 0001 / 0001 | LOSS 0.1985\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0001 / 0006 | LOSS 0.1842\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0002 / 0006 | LOSS 0.1968\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0003 / 0006 | LOSS 0.1938\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0004 / 0006 | LOSS 0.1929\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0005 / 0006 | LOSS 0.1951\n","TRAIN: EPOCH 0032 / 0100 | BATCH 0006 / 0006 | LOSS 0.1956\n","VALID: EPOCH 0032 / 0100 | BATCH 0001 / 0001 | LOSS 0.1884\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0001 / 0006 | LOSS 0.1842\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0002 / 0006 | LOSS 0.1882\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0003 / 0006 | LOSS 0.1895\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0004 / 0006 | LOSS 0.1890\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0005 / 0006 | LOSS 0.1892\n","TRAIN: EPOCH 0033 / 0100 | BATCH 0006 / 0006 | LOSS 0.1930\n","VALID: EPOCH 0033 / 0100 | BATCH 0001 / 0001 | LOSS 0.1987\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0001 / 0006 | LOSS 0.2016\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0002 / 0006 | LOSS 0.1924\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0003 / 0006 | LOSS 0.1919\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0004 / 0006 | LOSS 0.1922\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0005 / 0006 | LOSS 0.1910\n","TRAIN: EPOCH 0034 / 0100 | BATCH 0006 / 0006 | LOSS 0.1902\n","VALID: EPOCH 0034 / 0100 | BATCH 0001 / 0001 | LOSS 0.1872\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0001 / 0006 | LOSS 0.2034\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0002 / 0006 | LOSS 0.1975\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0003 / 0006 | LOSS 0.1928\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0004 / 0006 | LOSS 0.1957\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0005 / 0006 | LOSS 0.1909\n","TRAIN: EPOCH 0035 / 0100 | BATCH 0006 / 0006 | LOSS 0.1912\n","VALID: EPOCH 0035 / 0100 | BATCH 0001 / 0001 | LOSS 0.1870\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0001 / 0006 | LOSS 0.1800\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0002 / 0006 | LOSS 0.1810\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0003 / 0006 | LOSS 0.1905\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0004 / 0006 | LOSS 0.1876\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0005 / 0006 | LOSS 0.1913\n","TRAIN: EPOCH 0036 / 0100 | BATCH 0006 / 0006 | LOSS 0.1896\n","VALID: EPOCH 0036 / 0100 | BATCH 0001 / 0001 | LOSS 0.1988\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0001 / 0006 | LOSS 0.1823\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0002 / 0006 | LOSS 0.1835\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0003 / 0006 | LOSS 0.1908\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0004 / 0006 | LOSS 0.1920\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0005 / 0006 | LOSS 0.1902\n","TRAIN: EPOCH 0037 / 0100 | BATCH 0006 / 0006 | LOSS 0.1888\n","VALID: EPOCH 0037 / 0100 | BATCH 0001 / 0001 | LOSS 0.1822\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0001 / 0006 | LOSS 0.1886\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0002 / 0006 | LOSS 0.1911\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0003 / 0006 | LOSS 0.1872\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0004 / 0006 | LOSS 0.1832\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0005 / 0006 | LOSS 0.1859\n","TRAIN: EPOCH 0038 / 0100 | BATCH 0006 / 0006 | LOSS 0.1872\n","VALID: EPOCH 0038 / 0100 | BATCH 0001 / 0001 | LOSS 0.1874\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0001 / 0006 | LOSS 0.1859\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0002 / 0006 | LOSS 0.1910\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0003 / 0006 | LOSS 0.1887\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0004 / 0006 | LOSS 0.1863\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0005 / 0006 | LOSS 0.1847\n","TRAIN: EPOCH 0039 / 0100 | BATCH 0006 / 0006 | LOSS 0.1885\n","VALID: EPOCH 0039 / 0100 | BATCH 0001 / 0001 | LOSS 0.1778\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0001 / 0006 | LOSS 0.1951\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0002 / 0006 | LOSS 0.1933\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0003 / 0006 | LOSS 0.1905\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0004 / 0006 | LOSS 0.1926\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0005 / 0006 | LOSS 0.1896\n","TRAIN: EPOCH 0040 / 0100 | BATCH 0006 / 0006 | LOSS 0.1895\n","VALID: EPOCH 0040 / 0100 | BATCH 0001 / 0001 | LOSS 0.1910\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0001 / 0006 | LOSS 0.2029\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0002 / 0006 | LOSS 0.1931\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0003 / 0006 | LOSS 0.1923\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0004 / 0006 | LOSS 0.1897\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0005 / 0006 | LOSS 0.1930\n","TRAIN: EPOCH 0041 / 0100 | BATCH 0006 / 0006 | LOSS 0.1920\n","VALID: EPOCH 0041 / 0100 | BATCH 0001 / 0001 | LOSS 0.1844\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0001 / 0006 | LOSS 0.1905\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0002 / 0006 | LOSS 0.1882\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0003 / 0006 | LOSS 0.1881\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0004 / 0006 | LOSS 0.1886\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0005 / 0006 | LOSS 0.1875\n","TRAIN: EPOCH 0042 / 0100 | BATCH 0006 / 0006 | LOSS 0.1885\n","VALID: EPOCH 0042 / 0100 | BATCH 0001 / 0001 | LOSS 0.1895\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0001 / 0006 | LOSS 0.1986\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0002 / 0006 | LOSS 0.1850\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0003 / 0006 | LOSS 0.1864\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0004 / 0006 | LOSS 0.1857\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0005 / 0006 | LOSS 0.1888\n","TRAIN: EPOCH 0043 / 0100 | BATCH 0006 / 0006 | LOSS 0.1876\n","VALID: EPOCH 0043 / 0100 | BATCH 0001 / 0001 | LOSS 0.1895\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0001 / 0006 | LOSS 0.1743\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0002 / 0006 | LOSS 0.1833\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0003 / 0006 | LOSS 0.1861\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0004 / 0006 | LOSS 0.1861\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0005 / 0006 | LOSS 0.1869\n","TRAIN: EPOCH 0044 / 0100 | BATCH 0006 / 0006 | LOSS 0.1872\n","VALID: EPOCH 0044 / 0100 | BATCH 0001 / 0001 | LOSS 0.2008\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0001 / 0006 | LOSS 0.1806\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0002 / 0006 | LOSS 0.1837\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0003 / 0006 | LOSS 0.1819\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0004 / 0006 | LOSS 0.1853\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0005 / 0006 | LOSS 0.1852\n","TRAIN: EPOCH 0045 / 0100 | BATCH 0006 / 0006 | LOSS 0.1893\n","VALID: EPOCH 0045 / 0100 | BATCH 0001 / 0001 | LOSS 0.1764\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0001 / 0006 | LOSS 0.1724\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0002 / 0006 | LOSS 0.1744\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0003 / 0006 | LOSS 0.1813\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0004 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0005 / 0006 | LOSS 0.1853\n","TRAIN: EPOCH 0046 / 0100 | BATCH 0006 / 0006 | LOSS 0.1864\n","VALID: EPOCH 0046 / 0100 | BATCH 0001 / 0001 | LOSS 0.1828\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0001 / 0006 | LOSS 0.1991\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0002 / 0006 | LOSS 0.1963\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0003 / 0006 | LOSS 0.1912\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0004 / 0006 | LOSS 0.1861\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0005 / 0006 | LOSS 0.1833\n","TRAIN: EPOCH 0047 / 0100 | BATCH 0006 / 0006 | LOSS 0.1823\n","VALID: EPOCH 0047 / 0100 | BATCH 0001 / 0001 | LOSS 0.1816\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0001 / 0006 | LOSS 0.2230\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0002 / 0006 | LOSS 0.1991\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0003 / 0006 | LOSS 0.1928\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0004 / 0006 | LOSS 0.1916\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0005 / 0006 | LOSS 0.1903\n","TRAIN: EPOCH 0048 / 0100 | BATCH 0006 / 0006 | LOSS 0.1898\n","VALID: EPOCH 0048 / 0100 | BATCH 0001 / 0001 | LOSS 0.1822\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0001 / 0006 | LOSS 0.2018\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0002 / 0006 | LOSS 0.1870\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0003 / 0006 | LOSS 0.1827\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0004 / 0006 | LOSS 0.1818\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0005 / 0006 | LOSS 0.1794\n","TRAIN: EPOCH 0049 / 0100 | BATCH 0006 / 0006 | LOSS 0.1828\n","VALID: EPOCH 0049 / 0100 | BATCH 0001 / 0001 | LOSS 0.2057\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0001 / 0006 | LOSS 0.1871\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0002 / 0006 | LOSS 0.1809\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0003 / 0006 | LOSS 0.1764\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0004 / 0006 | LOSS 0.1816\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0005 / 0006 | LOSS 0.1846\n","TRAIN: EPOCH 0050 / 0100 | BATCH 0006 / 0006 | LOSS 0.1841\n","VALID: EPOCH 0050 / 0100 | BATCH 0001 / 0001 | LOSS 0.1787\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0001 / 0006 | LOSS 0.1808\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0002 / 0006 | LOSS 0.1766\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0003 / 0006 | LOSS 0.1783\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0004 / 0006 | LOSS 0.1833\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0005 / 0006 | LOSS 0.1868\n","TRAIN: EPOCH 0051 / 0100 | BATCH 0006 / 0006 | LOSS 0.1854\n","VALID: EPOCH 0051 / 0100 | BATCH 0001 / 0001 | LOSS 0.1778\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0001 / 0006 | LOSS 0.1948\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0002 / 0006 | LOSS 0.1826\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0003 / 0006 | LOSS 0.1840\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0004 / 0006 | LOSS 0.1819\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0005 / 0006 | LOSS 0.1790\n","TRAIN: EPOCH 0052 / 0100 | BATCH 0006 / 0006 | LOSS 0.1795\n","VALID: EPOCH 0052 / 0100 | BATCH 0001 / 0001 | LOSS 0.1786\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0001 / 0006 | LOSS 0.1822\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0002 / 0006 | LOSS 0.1771\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0003 / 0006 | LOSS 0.1787\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0004 / 0006 | LOSS 0.1797\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0005 / 0006 | LOSS 0.1788\n","TRAIN: EPOCH 0053 / 0100 | BATCH 0006 / 0006 | LOSS 0.1768\n","VALID: EPOCH 0053 / 0100 | BATCH 0001 / 0001 | LOSS 0.1820\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0001 / 0006 | LOSS 0.1901\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0002 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0003 / 0006 | LOSS 0.1869\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0004 / 0006 | LOSS 0.1808\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0005 / 0006 | LOSS 0.1827\n","TRAIN: EPOCH 0054 / 0100 | BATCH 0006 / 0006 | LOSS 0.1818\n","VALID: EPOCH 0054 / 0100 | BATCH 0001 / 0001 | LOSS 0.1850\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0001 / 0006 | LOSS 0.1600\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0002 / 0006 | LOSS 0.1739\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0003 / 0006 | LOSS 0.1762\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0004 / 0006 | LOSS 0.1764\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0005 / 0006 | LOSS 0.1770\n","TRAIN: EPOCH 0055 / 0100 | BATCH 0006 / 0006 | LOSS 0.1778\n","VALID: EPOCH 0055 / 0100 | BATCH 0001 / 0001 | LOSS 0.1798\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0001 / 0006 | LOSS 0.1748\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0002 / 0006 | LOSS 0.1869\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0003 / 0006 | LOSS 0.1800\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0004 / 0006 | LOSS 0.1779\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0005 / 0006 | LOSS 0.1836\n","TRAIN: EPOCH 0056 / 0100 | BATCH 0006 / 0006 | LOSS 0.1823\n","VALID: EPOCH 0056 / 0100 | BATCH 0001 / 0001 | LOSS 0.1851\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0001 / 0006 | LOSS 0.1795\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0002 / 0006 | LOSS 0.1759\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0003 / 0006 | LOSS 0.1764\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0004 / 0006 | LOSS 0.1797\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0005 / 0006 | LOSS 0.1790\n","TRAIN: EPOCH 0057 / 0100 | BATCH 0006 / 0006 | LOSS 0.1779\n","VALID: EPOCH 0057 / 0100 | BATCH 0001 / 0001 | LOSS 0.1834\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0001 / 0006 | LOSS 0.1738\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0002 / 0006 | LOSS 0.1962\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0003 / 0006 | LOSS 0.1893\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0004 / 0006 | LOSS 0.1867\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0005 / 0006 | LOSS 0.1864\n","TRAIN: EPOCH 0058 / 0100 | BATCH 0006 / 0006 | LOSS 0.1842\n","VALID: EPOCH 0058 / 0100 | BATCH 0001 / 0001 | LOSS 0.1903\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0001 / 0006 | LOSS 0.2027\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0002 / 0006 | LOSS 0.1829\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0003 / 0006 | LOSS 0.1850\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0004 / 0006 | LOSS 0.1794\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0005 / 0006 | LOSS 0.1798\n","TRAIN: EPOCH 0059 / 0100 | BATCH 0006 / 0006 | LOSS 0.1779\n","VALID: EPOCH 0059 / 0100 | BATCH 0001 / 0001 | LOSS 0.2022\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0001 / 0006 | LOSS 0.1712\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0002 / 0006 | LOSS 0.1675\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0003 / 0006 | LOSS 0.1693\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0004 / 0006 | LOSS 0.1714\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0005 / 0006 | LOSS 0.1713\n","TRAIN: EPOCH 0060 / 0100 | BATCH 0006 / 0006 | LOSS 0.1762\n","VALID: EPOCH 0060 / 0100 | BATCH 0001 / 0001 | LOSS 0.1903\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0001 / 0006 | LOSS 0.1869\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0002 / 0006 | LOSS 0.1862\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0003 / 0006 | LOSS 0.1806\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0004 / 0006 | LOSS 0.1769\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0005 / 0006 | LOSS 0.1758\n","TRAIN: EPOCH 0061 / 0100 | BATCH 0006 / 0006 | LOSS 0.1754\n","VALID: EPOCH 0061 / 0100 | BATCH 0001 / 0001 | LOSS 0.2197\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0001 / 0006 | LOSS 0.1736\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0002 / 0006 | LOSS 0.1741\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0003 / 0006 | LOSS 0.1683\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0004 / 0006 | LOSS 0.1761\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0005 / 0006 | LOSS 0.1752\n","TRAIN: EPOCH 0062 / 0100 | BATCH 0006 / 0006 | LOSS 0.1765\n","VALID: EPOCH 0062 / 0100 | BATCH 0001 / 0001 | LOSS 0.1758\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0001 / 0006 | LOSS 0.1834\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0002 / 0006 | LOSS 0.1772\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0003 / 0006 | LOSS 0.1785\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0004 / 0006 | LOSS 0.1777\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0005 / 0006 | LOSS 0.1819\n","TRAIN: EPOCH 0063 / 0100 | BATCH 0006 / 0006 | LOSS 0.1803\n","VALID: EPOCH 0063 / 0100 | BATCH 0001 / 0001 | LOSS 0.1867\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0001 / 0006 | LOSS 0.1682\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0002 / 0006 | LOSS 0.1710\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0003 / 0006 | LOSS 0.1723\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0004 / 0006 | LOSS 0.1703\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0005 / 0006 | LOSS 0.1725\n","TRAIN: EPOCH 0064 / 0100 | BATCH 0006 / 0006 | LOSS 0.1722\n","VALID: EPOCH 0064 / 0100 | BATCH 0001 / 0001 | LOSS 0.1796\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0001 / 0006 | LOSS 0.2018\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0002 / 0006 | LOSS 0.1826\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0003 / 0006 | LOSS 0.1818\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0004 / 0006 | LOSS 0.1768\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0005 / 0006 | LOSS 0.1748\n","TRAIN: EPOCH 0065 / 0100 | BATCH 0006 / 0006 | LOSS 0.1721\n","VALID: EPOCH 0065 / 0100 | BATCH 0001 / 0001 | LOSS 0.1810\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0001 / 0006 | LOSS 0.1614\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0002 / 0006 | LOSS 0.1652\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0003 / 0006 | LOSS 0.1653\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0004 / 0006 | LOSS 0.1653\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0005 / 0006 | LOSS 0.1693\n","TRAIN: EPOCH 0066 / 0100 | BATCH 0006 / 0006 | LOSS 0.1695\n","VALID: EPOCH 0066 / 0100 | BATCH 0001 / 0001 | LOSS 0.1747\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0001 / 0006 | LOSS 0.1654\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0002 / 0006 | LOSS 0.1641\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0003 / 0006 | LOSS 0.1684\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0004 / 0006 | LOSS 0.1685\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0005 / 0006 | LOSS 0.1699\n","TRAIN: EPOCH 0067 / 0100 | BATCH 0006 / 0006 | LOSS 0.1694\n","VALID: EPOCH 0067 / 0100 | BATCH 0001 / 0001 | LOSS 0.1861\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0001 / 0006 | LOSS 0.1712\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0002 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0003 / 0006 | LOSS 0.1628\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0004 / 0006 | LOSS 0.1681\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0005 / 0006 | LOSS 0.1692\n","TRAIN: EPOCH 0068 / 0100 | BATCH 0006 / 0006 | LOSS 0.1674\n","VALID: EPOCH 0068 / 0100 | BATCH 0001 / 0001 | LOSS 0.1857\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0001 / 0006 | LOSS 0.1649\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0002 / 0006 | LOSS 0.1606\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0003 / 0006 | LOSS 0.1580\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0004 / 0006 | LOSS 0.1612\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0005 / 0006 | LOSS 0.1640\n","TRAIN: EPOCH 0069 / 0100 | BATCH 0006 / 0006 | LOSS 0.1660\n","VALID: EPOCH 0069 / 0100 | BATCH 0001 / 0001 | LOSS 0.1737\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0001 / 0006 | LOSS 0.1734\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0002 / 0006 | LOSS 0.1752\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0003 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0004 / 0006 | LOSS 0.1678\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0005 / 0006 | LOSS 0.1701\n","TRAIN: EPOCH 0070 / 0100 | BATCH 0006 / 0006 | LOSS 0.1699\n","VALID: EPOCH 0070 / 0100 | BATCH 0001 / 0001 | LOSS 0.1798\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0001 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0002 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0003 / 0006 | LOSS 0.1701\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0004 / 0006 | LOSS 0.1676\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0005 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0071 / 0100 | BATCH 0006 / 0006 | LOSS 0.1710\n","VALID: EPOCH 0071 / 0100 | BATCH 0001 / 0001 | LOSS 0.1862\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0001 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0002 / 0006 | LOSS 0.1687\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0003 / 0006 | LOSS 0.1681\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0004 / 0006 | LOSS 0.1652\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0005 / 0006 | LOSS 0.1667\n","TRAIN: EPOCH 0072 / 0100 | BATCH 0006 / 0006 | LOSS 0.1697\n","VALID: EPOCH 0072 / 0100 | BATCH 0001 / 0001 | LOSS 0.1788\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0001 / 0006 | LOSS 0.1553\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0002 / 0006 | LOSS 0.1559\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0003 / 0006 | LOSS 0.1600\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0004 / 0006 | LOSS 0.1594\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0005 / 0006 | LOSS 0.1625\n","TRAIN: EPOCH 0073 / 0100 | BATCH 0006 / 0006 | LOSS 0.1668\n","VALID: EPOCH 0073 / 0100 | BATCH 0001 / 0001 | LOSS 0.1817\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0001 / 0006 | LOSS 0.1854\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0002 / 0006 | LOSS 0.1689\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0003 / 0006 | LOSS 0.1700\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0004 / 0006 | LOSS 0.1693\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0005 / 0006 | LOSS 0.1670\n","TRAIN: EPOCH 0074 / 0100 | BATCH 0006 / 0006 | LOSS 0.1652\n","VALID: EPOCH 0074 / 0100 | BATCH 0001 / 0001 | LOSS 0.1928\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0001 / 0006 | LOSS 0.1685\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0002 / 0006 | LOSS 0.1697\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0003 / 0006 | LOSS 0.1715\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0004 / 0006 | LOSS 0.1704\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0005 / 0006 | LOSS 0.1680\n","TRAIN: EPOCH 0075 / 0100 | BATCH 0006 / 0006 | LOSS 0.1675\n","VALID: EPOCH 0075 / 0100 | BATCH 0001 / 0001 | LOSS 0.1921\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0001 / 0006 | LOSS 0.1648\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0002 / 0006 | LOSS 0.1655\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0003 / 0006 | LOSS 0.1653\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0004 / 0006 | LOSS 0.1674\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0005 / 0006 | LOSS 0.1659\n","TRAIN: EPOCH 0076 / 0100 | BATCH 0006 / 0006 | LOSS 0.1647\n","VALID: EPOCH 0076 / 0100 | BATCH 0001 / 0001 | LOSS 0.1934\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0001 / 0006 | LOSS 0.1572\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0002 / 0006 | LOSS 0.1735\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0003 / 0006 | LOSS 0.1753\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0004 / 0006 | LOSS 0.1722\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0005 / 0006 | LOSS 0.1699\n","TRAIN: EPOCH 0077 / 0100 | BATCH 0006 / 0006 | LOSS 0.1668\n","VALID: EPOCH 0077 / 0100 | BATCH 0001 / 0001 | LOSS 0.1852\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0001 / 0006 | LOSS 0.1659\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0002 / 0006 | LOSS 0.1638\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0003 / 0006 | LOSS 0.1639\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0004 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0005 / 0006 | LOSS 0.1698\n","TRAIN: EPOCH 0078 / 0100 | BATCH 0006 / 0006 | LOSS 0.1672\n","VALID: EPOCH 0078 / 0100 | BATCH 0001 / 0001 | LOSS 0.1857\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0001 / 0006 | LOSS 0.1670\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0002 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0003 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0004 / 0006 | LOSS 0.1650\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0005 / 0006 | LOSS 0.1677\n","TRAIN: EPOCH 0079 / 0100 | BATCH 0006 / 0006 | LOSS 0.1669\n","VALID: EPOCH 0079 / 0100 | BATCH 0001 / 0001 | LOSS 0.1777\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0001 / 0006 | LOSS 0.1758\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0002 / 0006 | LOSS 0.1774\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0003 / 0006 | LOSS 0.1703\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0004 / 0006 | LOSS 0.1716\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0005 / 0006 | LOSS 0.1720\n","TRAIN: EPOCH 0080 / 0100 | BATCH 0006 / 0006 | LOSS 0.1708\n","VALID: EPOCH 0080 / 0100 | BATCH 0001 / 0001 | LOSS 0.1914\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0001 / 0006 | LOSS 0.1573\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0002 / 0006 | LOSS 0.1621\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0003 / 0006 | LOSS 0.1655\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0004 / 0006 | LOSS 0.1703\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0005 / 0006 | LOSS 0.1683\n","TRAIN: EPOCH 0081 / 0100 | BATCH 0006 / 0006 | LOSS 0.1676\n","VALID: EPOCH 0081 / 0100 | BATCH 0001 / 0001 | LOSS 0.1753\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0001 / 0006 | LOSS 0.1725\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0002 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0003 / 0006 | LOSS 0.1656\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0004 / 0006 | LOSS 0.1635\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0005 / 0006 | LOSS 0.1633\n","TRAIN: EPOCH 0082 / 0100 | BATCH 0006 / 0006 | LOSS 0.1626\n","VALID: EPOCH 0082 / 0100 | BATCH 0001 / 0001 | LOSS 0.1809\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0001 / 0006 | LOSS 0.1542\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0002 / 0006 | LOSS 0.1559\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0003 / 0006 | LOSS 0.1581\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0004 / 0006 | LOSS 0.1624\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0005 / 0006 | LOSS 0.1664\n","TRAIN: EPOCH 0083 / 0100 | BATCH 0006 / 0006 | LOSS 0.1652\n","VALID: EPOCH 0083 / 0100 | BATCH 0001 / 0001 | LOSS 0.1969\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0001 / 0006 | LOSS 0.1684\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0002 / 0006 | LOSS 0.1639\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0003 / 0006 | LOSS 0.1639\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0004 / 0006 | LOSS 0.1665\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0005 / 0006 | LOSS 0.1663\n","TRAIN: EPOCH 0084 / 0100 | BATCH 0006 / 0006 | LOSS 0.1662\n","VALID: EPOCH 0084 / 0100 | BATCH 0001 / 0001 | LOSS 0.1818\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0001 / 0006 | LOSS 0.1602\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0002 / 0006 | LOSS 0.1593\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0003 / 0006 | LOSS 0.1588\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0004 / 0006 | LOSS 0.1549\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0005 / 0006 | LOSS 0.1597\n","TRAIN: EPOCH 0085 / 0100 | BATCH 0006 / 0006 | LOSS 0.1627\n","VALID: EPOCH 0085 / 0100 | BATCH 0001 / 0001 | LOSS 0.1847\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0001 / 0006 | LOSS 0.1649\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0002 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0003 / 0006 | LOSS 0.1624\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0004 / 0006 | LOSS 0.1614\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0005 / 0006 | LOSS 0.1656\n","TRAIN: EPOCH 0086 / 0100 | BATCH 0006 / 0006 | LOSS 0.1645\n","VALID: EPOCH 0086 / 0100 | BATCH 0001 / 0001 | LOSS 0.1776\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0001 / 0006 | LOSS 0.1502\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0002 / 0006 | LOSS 0.1499\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0003 / 0006 | LOSS 0.1520\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0004 / 0006 | LOSS 0.1582\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0005 / 0006 | LOSS 0.1579\n","TRAIN: EPOCH 0087 / 0100 | BATCH 0006 / 0006 | LOSS 0.1597\n","VALID: EPOCH 0087 / 0100 | BATCH 0001 / 0001 | LOSS 0.1775\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0001 / 0006 | LOSS 0.1565\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0002 / 0006 | LOSS 0.1529\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0003 / 0006 | LOSS 0.1581\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0004 / 0006 | LOSS 0.1561\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0005 / 0006 | LOSS 0.1609\n","TRAIN: EPOCH 0088 / 0100 | BATCH 0006 / 0006 | LOSS 0.1604\n","VALID: EPOCH 0088 / 0100 | BATCH 0001 / 0001 | LOSS 0.1825\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0001 / 0006 | LOSS 0.1605\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0002 / 0006 | LOSS 0.1596\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0003 / 0006 | LOSS 0.1594\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0004 / 0006 | LOSS 0.1611\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0005 / 0006 | LOSS 0.1621\n","TRAIN: EPOCH 0089 / 0100 | BATCH 0006 / 0006 | LOSS 0.1618\n","VALID: EPOCH 0089 / 0100 | BATCH 0001 / 0001 | LOSS 0.1866\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0001 / 0006 | LOSS 0.1554\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0002 / 0006 | LOSS 0.1592\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0003 / 0006 | LOSS 0.1631\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0004 / 0006 | LOSS 0.1648\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0005 / 0006 | LOSS 0.1632\n","TRAIN: EPOCH 0090 / 0100 | BATCH 0006 / 0006 | LOSS 0.1609\n","VALID: EPOCH 0090 / 0100 | BATCH 0001 / 0001 | LOSS 0.1912\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0001 / 0006 | LOSS 0.1560\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0002 / 0006 | LOSS 0.1587\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0003 / 0006 | LOSS 0.1563\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0004 / 0006 | LOSS 0.1671\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0005 / 0006 | LOSS 0.1728\n","TRAIN: EPOCH 0091 / 0100 | BATCH 0006 / 0006 | LOSS 0.1686\n","VALID: EPOCH 0091 / 0100 | BATCH 0001 / 0001 | LOSS 0.1767\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0001 / 0006 | LOSS 0.1558\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0002 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0003 / 0006 | LOSS 0.1580\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0004 / 0006 | LOSS 0.1588\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0005 / 0006 | LOSS 0.1619\n","TRAIN: EPOCH 0092 / 0100 | BATCH 0006 / 0006 | LOSS 0.1616\n","VALID: EPOCH 0092 / 0100 | BATCH 0001 / 0001 | LOSS 0.1759\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0001 / 0006 | LOSS 0.1498\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0002 / 0006 | LOSS 0.1554\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0003 / 0006 | LOSS 0.1561\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0004 / 0006 | LOSS 0.1601\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0005 / 0006 | LOSS 0.1591\n","TRAIN: EPOCH 0093 / 0100 | BATCH 0006 / 0006 | LOSS 0.1586\n","VALID: EPOCH 0093 / 0100 | BATCH 0001 / 0001 | LOSS 0.1789\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0001 / 0006 | LOSS 0.1466\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0002 / 0006 | LOSS 0.1486\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0003 / 0006 | LOSS 0.1565\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0004 / 0006 | LOSS 0.1562\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0005 / 0006 | LOSS 0.1574\n","TRAIN: EPOCH 0094 / 0100 | BATCH 0006 / 0006 | LOSS 0.1575\n","VALID: EPOCH 0094 / 0100 | BATCH 0001 / 0001 | LOSS 0.1818\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0001 / 0006 | LOSS 0.1603\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0002 / 0006 | LOSS 0.1597\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0003 / 0006 | LOSS 0.1575\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0004 / 0006 | LOSS 0.1551\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0005 / 0006 | LOSS 0.1565\n","TRAIN: EPOCH 0095 / 0100 | BATCH 0006 / 0006 | LOSS 0.1561\n","VALID: EPOCH 0095 / 0100 | BATCH 0001 / 0001 | LOSS 0.1774\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0001 / 0006 | LOSS 0.1430\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0002 / 0006 | LOSS 0.1469\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0003 / 0006 | LOSS 0.1483\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0004 / 0006 | LOSS 0.1504\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0005 / 0006 | LOSS 0.1561\n","TRAIN: EPOCH 0096 / 0100 | BATCH 0006 / 0006 | LOSS 0.1564\n","VALID: EPOCH 0096 / 0100 | BATCH 0001 / 0001 | LOSS 0.1734\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0001 / 0006 | LOSS 0.1626\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0002 / 0006 | LOSS 0.1620\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0003 / 0006 | LOSS 0.1604\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0004 / 0006 | LOSS 0.1613\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0005 / 0006 | LOSS 0.1570\n","TRAIN: EPOCH 0097 / 0100 | BATCH 0006 / 0006 | LOSS 0.1557\n","VALID: EPOCH 0097 / 0100 | BATCH 0001 / 0001 | LOSS 0.1801\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0001 / 0006 | LOSS 0.1635\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0002 / 0006 | LOSS 0.1639\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0003 / 0006 | LOSS 0.1625\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0004 / 0006 | LOSS 0.1601\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0005 / 0006 | LOSS 0.1599\n","TRAIN: EPOCH 0098 / 0100 | BATCH 0006 / 0006 | LOSS 0.1617\n","VALID: EPOCH 0098 / 0100 | BATCH 0001 / 0001 | LOSS 0.1938\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0001 / 0006 | LOSS 0.1401\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0002 / 0006 | LOSS 0.1472\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0003 / 0006 | LOSS 0.1550\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0004 / 0006 | LOSS 0.1554\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0005 / 0006 | LOSS 0.1544\n","TRAIN: EPOCH 0099 / 0100 | BATCH 0006 / 0006 | LOSS 0.1560\n","VALID: EPOCH 0099 / 0100 | BATCH 0001 / 0001 | LOSS 0.1788\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0001 / 0006 | LOSS 0.1553\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0002 / 0006 | LOSS 0.1583\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0003 / 0006 | LOSS 0.1581\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0004 / 0006 | LOSS 0.1556\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0005 / 0006 | LOSS 0.1565\n","TRAIN: EPOCH 0100 / 0100 | BATCH 0006 / 0006 | LOSS 0.1555\n","VALID: EPOCH 0100 / 0100 | BATCH 0001 / 0001 | LOSS 0.1797\n"]}],"source":["!python3 '/home/work/deep/xai/겨울방학스터디/UNet/train.py'"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","executionInfo":{"elapsed":7969,"status":"ok","timestamp":1584314467883,"user":{"displayName":"한요섭","photoUrl":"https://lh5.googleusercontent.com/-YFCq7U3ZRkA/AAAAAAAAAAI/AAAAAAAARHc/gXM5cCKzru0/s64/photo.jpg","userId":"13321053678363808287"},"user_tz":360},"id":"udJh_9lGeDrF","outputId":"642860fa-7b23-4e51-cc53-cec3538bb1a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-24 11:34:16.421720: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-24 11:34:16.421823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-24 11:34:16.423150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-24 11:34:16.430148: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-24 11:34:17.408382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Traceback (most recent call last):\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/eval.py\", line 12, in <module>\n","    from torchvision import transforms, datasets\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\", line 6, in <module>\n","    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\", line 2, in <module>\n","    from .convnext import *\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\", line 8, in <module>\n","    from ..ops.misc import Conv2dNormActivation, Permute\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\", line 23, in <module>\n","    from .poolers import MultiScaleRoIAlign\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/poolers.py\", line 10, in <module>\n","    from .roi_align import roi_align\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/ops/roi_align.py\", line 4, in <module>\n","    import torch._dynamo\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\", line 2, in <module>\n","    from . import allowed_functions, convert_frame, eval_frame, resume_execution\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 48, in <module>\n","    from .symbolic_convert import InstructionTranslator\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in <module>\n","    class InstructionTranslatorBase(Checkpointable[InstructionTranslatorGraphState]):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1668, in InstructionTranslatorBase\n","    INPLACE_REMAINDER = stack_op(operator.imod)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 163, in stack_op\n","    nargs = len(inspect.signature(fn).parameters)\n","  File \"/usr/lib/python3.10/inspect.py\", line 3254, in signature\n","    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n","  File \"/usr/lib/python3.10/inspect.py\", line 3002, in from_callable\n","    return _signature_from_callable(obj, sigcls=cls,\n","  File \"/usr/lib/python3.10/inspect.py\", line 2468, in _signature_from_callable\n","    return _signature_from_builtin(sigcls, obj,\n","  File \"/usr/lib/python3.10/inspect.py\", line 2277, in _signature_from_builtin\n","    return _signature_fromstr(cls, func, s, skip_bound_arg)\n","  File \"/usr/lib/python3.10/inspect.py\", line 2152, in _signature_fromstr\n","    sys_module_dict = sys.modules.copy()\n","KeyboardInterrupt\n"]}],"source":["!python3 '/home/work/deep/xai/겨울방학스터디/UNet/eval.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","executionInfo":{"elapsed":12442,"status":"ok","timestamp":1584852126774,"user":{"displayName":"한요섭","photoUrl":"https://lh5.googleusercontent.com/-YFCq7U3ZRkA/AAAAAAAAAAI/AAAAAAAARHc/gXM5cCKzru0/s64/photo.jpg","userId":"13321053678363808287"},"user_tz":360},"id":"pz7ioSlzeVeN","outputId":"c569fe40-0776-4c5f-9097-c1fece9a14f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-24 11:12:05.410942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-24 11:12:05.411046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-24 11:12:05.412398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-24 11:12:05.419413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-24 11:12:06.391601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-01\n","batch size: 4\n","number of epoch: 512\n","data dir: /home/work/deep/xai/겨울방학스터디/UNet/datasets\n","ckpt dir: /home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\n","log dir: /home/work/deep/xai/겨울방학스터디/UNet/log_v2\n","result dir: /home/work/deep/xai/겨울방학스터디/UNet/result_v2\n","mode: test\n","Traceback (most recent call last):\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\", line 94, in <module>\n","    dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/dataset.py\", line 16, in __init__\n","    lst_data = os.listdir(self.data_dir)\n","FileNotFoundError: [Errno 2] No such file or directory: '/home/work/deep/xai/겨울방학스터디/UNet/datasets/test'\n"]}],"source":["!python3 \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\" \\\n","--lr 1e-1 --batch_size 4 --num_epoch 512 \\\n","--data_dir \"/home/work/deep/xai/겨울방학스터디/UNet/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/겨울방학스터디/UNet/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/겨울방학스터디/UNet/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"CVoZZbqBbGcR"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-24 11:12:12.088043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-24 11:12:12.088143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-24 11:12:12.089460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-24 11:12:12.096507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-24 11:12:13.074074: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-02\n","batch size: 8\n","number of epoch: 256\n","data dir: /home/work/deep/xai/겨울방학스터디/UNet/datasets\n","ckpt dir: /home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\n","log dir: /home/work/deep/xai/겨울방학스터디/UNet/log_v2\n","result dir: /home/work/deep/xai/겨울방학스터디/UNet/result_v2\n","mode: test\n","Traceback (most recent call last):\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\", line 94, in <module>\n","    dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/dataset.py\", line 16, in __init__\n","    lst_data = os.listdir(self.data_dir)\n","FileNotFoundError: [Errno 2] No such file or directory: '/home/work/deep/xai/겨울방학스터디/UNet/datasets/test'\n"]}],"source":["!python3 \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\" \\\n","--lr 1e-2 --batch_size 8 --num_epoch 256 \\\n","--data_dir \"/home/work/deep/xai/겨울방학스터디/UNet/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/겨울방학스터디/UNet/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/겨울방학스터디/UNet/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-01-24 11:12:18.646866: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2025-01-24 11:12:18.646969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2025-01-24 11:12:18.648307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-01-24 11:12:18.655336: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-01-24 11:12:19.646470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","learning rate: 1.0000e-03\n","batch size: 2\n","number of epoch: 2048\n","data dir: /home/work/deep/xai/겨울방학스터디/UNet/datasets\n","ckpt dir: /home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\n","log dir: /home/work/deep/xai/겨울방학스터디/UNet/log_v2\n","result dir: /home/work/deep/xai/겨울방학스터디/UNet/result_v2\n","mode: test\n","Traceback (most recent call last):\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\", line 94, in <module>\n","    dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n","  File \"/home/work/deep/xai/겨울방학스터디/UNet/dataset.py\", line 16, in __init__\n","    lst_data = os.listdir(self.data_dir)\n","FileNotFoundError: [Errno 2] No such file or directory: '/home/work/deep/xai/겨울방학스터디/UNet/datasets/test'\n"]}],"source":["!python3 \"/home/work/deep/xai/겨울방학스터디/UNet/train.py\" \\\n","--lr 1e-3 --batch_size 2 --num_epoch 2048 \\\n","--data_dir \"/home/work/deep/xai/겨울방학스터디/UNet/datasets\" \\\n","--ckpt_dir \"/home/work/deep/xai/겨울방학스터디/UNet/checkpoint_v2\" \\\n","--log_dir \"/home/work/deep/xai/겨울방학스터디/UNet/log_v2\" \\\n","--result_dir \"/home/work/deep/xai/겨울방학스터디/UNet/result_v2\" \\\n","--mode \"test\" \\\n","--train_continue \"off\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOhn0PSXfBvXCiYxsnaoSU4","collapsed_sections":[],"mount_file_id":"1cFB-vCtcEHFjk1t7cXHJHg0Nog4LiOMZ","name":"run_unet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
